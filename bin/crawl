#!/usr/bin/env python

import sys
import argparse
import logging
import multiprocessing
import pymongo

from libbabywalk.db import get_request, set_completed
from libbabywalk.fetch import fetch_and_upload


WORKER_STATE = None


def main():

    parser = argparse.ArgumentParser()
    parser.add_argument('--verbose', '-v', action='count', default=0)
    parser.add_argument('--tempdir', metavar='<directory>', required=True)
    args = parser.parse_args()

    logging_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(format='crawl: %(levelname)s: %(message)s',
                        level=logging_level)

    pool = multiprocessing.Pool(200, initializer=worker_init)
    pool.imap_unordered(worker, generator(args.tempdir))
    pool.close()
    pool.join()


def generator(work_dir):

    client = pymongo.MongoClient()
    db = client.crawling

    while True:
        request = get_request(db)
        if request:
            yield {'request': request, 'work_dir': work_dir}
        else:
            break


def worker_init():

    mongo = pymongo.MongoClient()

    import boto3
    boto3.set_stream_logger('boto3.resources', logging.CRITICAL)
    aws_s3 = boto3.resource('s3')

    global WORKER_STATE
    WORKER_STATE = (mongo, aws_s3)


def worker(item):

    global WORKER_STATE
    (mongo, aws_s3) = WORKER_STATE

    for request, result in fetch_and_upload(item['request'], item['work_dir'], aws_s3):
        logging.info('result: %s for %s', result, request['fetch']['url'])
        set_completed(mongo.crawling, request, result)


if __name__ == '__main__':
    sys.exit(main())
