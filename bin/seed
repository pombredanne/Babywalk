#!/usr/bin/env python

import sys
import argparse
import logging
import pika
import json


def main():

    parser = argparse.ArgumentParser()
    parser.add_argument('--verbose', '-v', action='count', default=0)
    parser.add_argument('--bucket', metavar='<bucket>', required=True)
    parser.add_argument('--input',
                        metavar='<file>',
                        dest='seeds',
                        required=True)
    parser.add_argument('--depth', metavar='<number>', required=True)
    args = parser.parse_args()

    logging_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(format='seed: %(levelname)s: %(message)s',
                        level=logging_level)

    queue_name = 'crawler-input'

    connection = pika.BlockingConnection(pika.ConnectionParameters())

    channel = connection.channel()

    logging.info('creating queue %s', queue_name)
    channel.queue_declare(queue=queue_name,
                          durable=True,
                          exclusive=False,
                          auto_delete=False)

    generator = ({
        'seed': row[0],
        'uid': row[1],
        'depth': args.depth,
        'bucket': args.bucket
    } for row in inputs(args.seeds))

    for message in generator:
        channel.basic_publish(exchange='',
                              routing_key=queue_name,
                              body=json.dumps(message,
                                              ensure_ascii=False),
                              properties=pika.BasicProperties(
                                  content_type='application/json'))
        logging.info('sending message %s', message)

    connection.close()


def inputs(filename):

    with open(filename, mode='r', encoding='utf-8') as handle:
        for line in handle:
            logging.debug('line: %s', line)
            yield line.strip().split('\t')


if __name__ == '__main__':
    sys.exit(main())
